1:"$Sreact.fragment"
2:I[9304,["874","static/chunks/874-564943c90346e675.js","497","static/chunks/497-8f454888f232b3fa.js","766","static/chunks/766-98bd1540b448b2b9.js","177","static/chunks/app/layout-83c85cf5db64605f.js"],"ThemeProvider"]
3:I[9578,["874","static/chunks/874-564943c90346e675.js","497","static/chunks/497-8f454888f232b3fa.js","766","static/chunks/766-98bd1540b448b2b9.js","177","static/chunks/app/layout-83c85cf5db64605f.js"],"default"]
4:I[7555,[],""]
5:I[1295,[],""]
6:I[3063,["874","static/chunks/874-564943c90346e675.js","497","static/chunks/497-8f454888f232b3fa.js","766","static/chunks/766-98bd1540b448b2b9.js","177","static/chunks/app/layout-83c85cf5db64605f.js"],"Image"]
7:I[8659,["874","static/chunks/874-564943c90346e675.js","497","static/chunks/497-8f454888f232b3fa.js","663","static/chunks/663-bf703af64886d2e2.js","766","static/chunks/766-98bd1540b448b2b9.js","974","static/chunks/app/page-b0a112a1deee7e7b.js"],"default"]
8:I[9483,["874","static/chunks/874-564943c90346e675.js","497","static/chunks/497-8f454888f232b3fa.js","663","static/chunks/663-bf703af64886d2e2.js","766","static/chunks/766-98bd1540b448b2b9.js","974","static/chunks/app/page-b0a112a1deee7e7b.js"],"default"]
9:I[5316,["874","static/chunks/874-564943c90346e675.js","497","static/chunks/497-8f454888f232b3fa.js","663","static/chunks/663-bf703af64886d2e2.js","766","static/chunks/766-98bd1540b448b2b9.js","974","static/chunks/app/page-b0a112a1deee7e7b.js"],"default"]
a:I[7043,["874","static/chunks/874-564943c90346e675.js","497","static/chunks/497-8f454888f232b3fa.js","663","static/chunks/663-bf703af64886d2e2.js","766","static/chunks/766-98bd1540b448b2b9.js","974","static/chunks/app/page-b0a112a1deee7e7b.js"],"default"]
b:I[8300,["874","static/chunks/874-564943c90346e675.js","497","static/chunks/497-8f454888f232b3fa.js","663","static/chunks/663-bf703af64886d2e2.js","766","static/chunks/766-98bd1540b448b2b9.js","974","static/chunks/app/page-b0a112a1deee7e7b.js"],"default"]
c:I[1350,["874","static/chunks/874-564943c90346e675.js","497","static/chunks/497-8f454888f232b3fa.js","663","static/chunks/663-bf703af64886d2e2.js","766","static/chunks/766-98bd1540b448b2b9.js","974","static/chunks/app/page-b0a112a1deee7e7b.js"],"default"]
f:I[5038,["874","static/chunks/874-564943c90346e675.js","497","static/chunks/497-8f454888f232b3fa.js","663","static/chunks/663-bf703af64886d2e2.js","766","static/chunks/766-98bd1540b448b2b9.js","974","static/chunks/app/page-b0a112a1deee7e7b.js"],"default"]
14:I[5768,["874","static/chunks/874-564943c90346e675.js","497","static/chunks/497-8f454888f232b3fa.js","663","static/chunks/663-bf703af64886d2e2.js","766","static/chunks/766-98bd1540b448b2b9.js","974","static/chunks/app/page-b0a112a1deee7e7b.js"],"default"]
15:I[9665,[],"OutletBoundary"]
18:I[9665,[],"ViewportBoundary"]
1a:I[9665,[],"MetadataBoundary"]
1c:I[6614,[],""]
:HL["/_next/static/media/a34f9d1faa5f3315-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/css/730b9168ceaa3868.css","style"]
:HL["/_next/static/css/b3cbcd051438d1d5.css","style"]
d:T8fc,
## Overview

I recently built my personal portfolio website over two days, focusing on keeping the process straightforward and efficient. Rather than overcomplicating the project, I aimed to create a clear, maintainable site that represents my work accurately.

## Tech Stack and Workflow

- **Frontend Hosting:** [AWS Amplify](https://aws.amazon.com/amplify/)
- **Domain Management:** [AWS Route 53](https://aws.amazon.com/route53/)
- **Email (Contact Form & Newsletter):** [Resend](https://resend.com/)
- **UI Design & Component Styling:** [v0.dev](https://v0.dev/) + [Shadcn UI](https://ui.shadcn.dev/)
- **CSS Framework:** [Tailwind CSS](https://tailwindcss.com/)
- **Framework:** [Next.js (App Router)](https://nextjs.org/)

### Key Features

- Fully responsive design
- Blog and Projects that are easily editable
- Markdown files converted to HTML for blog and project content to avoid repetitive HTML/CSS work
- Animated bird flying across the top page
- Smooth parallax backgrounds and scroll effects
- Contact form and newsletter signup integrated with Resend
- Clean and simple UI, composed with v0.dev and Shadcn components
- Custom domain set up through AWS Route 53

## Development Breakdown

- **Day 1:**
  - Defined the UI layout and page structure
  - Adjusted and finalized the UI using v0.dev and Shadcn components
  - Set up TailwindCSS and basic styling

- **Day 2:**
  - Deployed the frontend to AWS Amplify
  - Linked the contact form and newsletter features to Resend
  - Configured a custom domain using AWS Route 53
  - Finalized minor styling adjustments and tested responsiveness

The two-day development timeline was manageable because of a straightforward workflow and familiarity with the tools used. Even with some additional refinements, the project could easily be completed within three days.

## Reflections

This project demonstrates my ability to:

- Build a complete and maintainable site within a short time frame
- Focus on clarity and simplicity in both design and development
- Integrate external services like Resend smoothly
- Handle domain setup, project deployment, and site operation independently

If you're thinking about creating your own portfolio and would like some help, feel free to reach out. I'd be happy to discuss and support your project.
e:T9cc,
# Learning Singapore‚Äôs Birds by Ear: A Look at the SingBirds Call Quiz

## Overview

SingBirds Call Quiz is an interactive web application that challenges players to identify bird species by their songs. The game is centered around birds found in **Singapore**, making it both a fun and educational experience.

Players view a **spectrogram** of a bird call and listen to the audio recording before choosing the correct bird species from multiple-choice options. After submitting an answer, they can view detailed information and a photo of the bird.

---

## Features

- üéß **Bird Call Audio**: Listen to real bird recordings.
- üìà **Spectrogram Display**: Visualize bird calls using spectrograms.
- ‚ùì **Multiple-Choice Questions**: Pick the correct species from a list of options.
- üìù **Detailed Feedback**:
  - See which birds you identified correctly or incorrectly.
  - Access bird information via Wikipedia API after answering.
  - View bird images dynamically pulled from Wikipedia.
- üåé **Hotspot and Country Selection**: Select specific countries (currently only Singapore) and birding hotspots.
- üéöÔ∏è **Adjustable Quiz Settings**: Set the number of questions.
- üìä **Quiz Result Summary**: Review your score, accuracy, and species performance.

---

## Tech Stack

### Frontend
- [React](https://react.dev/)
- [Tailwind CSS](https://tailwindcss.com/) for styling

### Backend
- [Django](https://www.djangoproject.com/) with Django REST Framework
- Custom APIs serving species lists, hotspot data, and bird observation records

### Audio Processing
- [Librosa](https://librosa.org/) for audio analysis
- [Matplotlib](https://matplotlib.org/) for spectrogram generation

---

## Data Sources

- **eBird API**:
  - Retrieves Singapore hotspots
  - Gets bird species observed at hotspots over the past 30 days (snapshot taken in October)

- **Xeno-Canto API**:
  - Provides bird song recordings
  - Filters recordings with **Quality A** and durations between **0‚Äì30 seconds**

- **Wikipedia API**:
  - Fetches species descriptions and images dynamically

---

## App Flow

1. Player selects a country (currently only Singapore) and a hotspot.
2. App retrieves a list of birds based on recent observations (snapshot taken in Oct 2024).
3. Spectrogram and audio are displayed.
4. Player submits an answer.
5. App shows feedback, Wikipedia information, and bird photo.
6. After the quiz, a final score and breakdown are shown.



Made with ‚ù§Ô∏è for bird lovers around the world.


10:T32f0,

## Introduction

This report synthesizes the current literature on training bird sound classification models in data-sparse regional contexts with an emphasis on Singapore or similar Southeast Asian biodiversity hotspots. Overall, the reviewed literature indicates that while many studies develop robust machine learning frameworks using deep convolutional neural networks (CNNs), transfer learning, and semi-supervised techniques for bird sound classification, only a subset of these works explicitly focus on or have been evaluated using Singapore-specific soundscape recordings.

## 1. General Frameworks and Methods in Data-Sparse Contexts

A number of recent studies address the common challenges inherent in bioacoustic monitoring when labeled audio data are scarce. Researchers have proposed solutions that leverage transfer learning from large global datasets, employ data augmentation techniques, and utilize weak or semi-supervised learning approaches to overcome the limited availability of high-quality regional audio samples. For example, Bellafkir et al. (2023) describe a CNN-based approach that uses data augmentation methods such as the addition of background noise and selective mixup to mitigate class imbalances that naturally arise in long-tail distributions. Their methodology‚Äîdeveloped originally for challenging soundscapes that include overlapping environmental noises and weak labels‚Äîis broadly applicable to scenarios with limited training data, such as those encountered in Southeast Asian urban and forest environments.

Deep transfer learning has been widely adopted to address the data limitation problem. Das et al. (2023) review various CNN architectures, including ResNet, Inception-v3, VGG, MobileNet, DenseNet, and EfficientNet, and detail how pre-trained networks (often on large-scale image or audio datasets) can be fine-tuned on much smaller regional datasets. These approaches considerably reduce the need for large quantities of labeled examples and decrease the training time while improving accuracy. Although the reviewed literature does not always include explicit evaluations using Singapore-specific data, the underlying methodologies show promise for adaptation to any data-sparse, biodiversity-rich region.

In addition to transfer learning, several groups have explored semi-supervised and weakly supervised approaches. For instance, Caprioli's (2022) work on semi-supervised classification using the FixMatch algorithm demonstrates that even with as few as 11 labeled samples per class, significant accuracy improvements can be attained compared to purely supervised baselines. These techniques are particularly relevant in contexts where field-collected labeled audio is extremely limited, such as in remote or urban ecosystems.

## 2. Challenges in Contexts Like Singapore

Many studies echo similar challenges in limited regional training data, especially when the targeted species are either rare or difficult to record. Common issues include:

- **Limited Availability of High-Quality Labeled Audio Data.** In the field, particularly in highly diverse tropical settings like Singapore, obtaining long-duration recordings with high signal-to-noise ratios is challenging. Critically endangered or resident species often have very sparse recordings, and collector efforts are constrained by practical limitations as well as the fine-grained differences in acoustic patterns (Bellafkir et al., 2023).

- **High Acoustic Similarity Among Coexisting Species.** The coexistence of many species within a small geographical area results in considerable overlap in acoustic signals. This situation demands models that can differentiate subtle variations in call structure and temporal vocalization patterns, which in turn requires inventive signal processing and advanced network architectures capable of capturing both spectral and temporal features (Das et al., 2023).

- **Environmental Noise and Overlapping Vocalizations.** Urban and forest settings in Southeast Asia, including Singapore, are characterized by background noises from both natural and anthropogenic sources. This not only complicates the audio signal but also increases the chance of false positives. Several studies address these problems through sophisticated post-processing techniques, such as class-wise threshold calibrations and confidence penalization for overly common species (Bellafkir et al., 2023).

## 3. Efforts Focused on Singaporean Soundscapes

When it comes to directly addressing the Singapore context, there is clear evidence of published work that has explicitly targeted Singapore's unique acoustic environment. Notably, Hexeberg et al. (2025) propose a semi-supervised classification approach for bird vocalizations that is explicitly tested using recordings from Singapore. Their work involves acoustic recordings from local sites‚Äîincluding the Singapore Botanic Gardens‚Äîcollected over extended periods. This study demonstrates the feasibility of detecting species with overlapping vocalizations and noisy backgrounds in an urban tropical environment by achieving reasonable precision on classes even when trained on very few labeled samples. In further experiments, their classifier is shown to reflect natural diurnal and nocturnal behavioral patterns and to manage site-specific variations effectively. Such results provide strong validation that semi-supervised approaches can be adapted successfully to manage the synthesis of regional data in Singapore.

Additionally, while Bellafkir et al. (2023) do not explicitly test their model on Singapore data, they note that the efficiency of their pipeline in handling noisy, imbalanced, and weakly labeled datasets makes it well-suited for deployment in tropical urban and forest settings such as Singapore. Thus, although not every study targets Singapore directly, many proposed frameworks are clearly applicable to similar contexts in Southeast Asia.

Another indirect contribution comes from studies that have been conducted in neighboring regions‚Äîsuch as the SiulMalaya dataset for Malaysia‚Äîwhich similarly face challenges of high species diversity and background noise in lowland forests (Jamil et al., 2023). While the SiulMalaya dataset is specific to Malaysia, the underlying machine learning approaches can offer valuable insights for adapting models to the Singaporean context, given the ecological and acoustic similarities among tropical Southeast Asian habitats.

## 4. Mitigation Strategies in Data-Sparse Settings

Several strategies are recommended to address data scarcity in regional contexts such as Singapore:

- **Transfer Learning and Region-Specific Fine-Tuning.** Numerous studies recommend using globally trained models and then fine-tuning them on localized, limited datasets. The use of pre-trained backbones such as EfficientNet (Ansar et al., 2024) and ResNet (Zhong et al., 2021) has been demonstrated to be beneficial when adapting to rare or underrepresented bird sounds. Although many works do not explicitly mention Singapore, the strategies described can be directly applied by incorporating local recording data during the fine-tuning stage.

- **Semi-Supervised and Weakly-Supervised Learning.** In environments where obtaining numerous labels is impractical, semi-supervised approaches that combine small amounts of labeled data with large volumes of unlabeled data provide a promising avenue. Hexeberg et al.'s (2025) work on semi-supervised classification in Singapore demonstrates that even with minimal labeled data, reasonable performance levels can be attained. Similarly, Caprioli's (2022) study on the FixMatch algorithm shows further improvements when pseudo-labels are generated from unlabeled recordings.

- **Data Augmentation and Synthetic Data Generation.** To supplement sparse datasets, augmentation methods such as time stretching, pitch shifting, and mixup are frequently applied. Bellafkir et al. (2023) incorporate several augmentation techniques designed to simulate the presence of background noise and overcome class imbalances. These practices are crucial in developing models that remain robust against the acoustic variability typical of urban and forested soundscapes in Singapore.

## 5. Gaps in the Current Literature

While several frameworks have been successfully applied in data-sparse contexts throughout Southeast Asia, there remains a relative paucity of models that have been exclusively trained and validated on Singapore-specific data. Das et al. (2023) note that many deep transfer learning-based studies rely on datasets sourced from global repositories such as Xeno-canto and BirdCLEF, and that there is no explicit reference to Singaporean bird audio recordings within these published efforts. This observation highlights a meaningful gap in the literature: the need for dedicated Singapore-specific datasets and models that account for the unique acoustic challenges, species assemblages, and environmental characteristics within Singapore. Future work in this area could benefit from developing comprehensive local datasets, combining the strengths of both supervised and unsupervised methods, and leveraging metadata (e.g., spatial-temporal information) that might further differentiate the subtle acoustic nuances among local species.

## 6. Conclusion

In summary, while there is a substantial body of literature that presents methodologies for training bird sound classification models in data-sparse contexts, only select studies have directly tackled the Singapore setting. The work of Hexeberg et al. (2025) clearly demonstrates that a semi-supervised classification approach using Singapore-specific recordings is feasible even in the presence of noise, overlapping calls, and limited labels. Other frameworks developed by Bellafkir et al. (2023) and those based on deep transfer learning by Das et al. (2023) offer methods that are inherently adaptable to Singapore; however, they often rely on data not collected specifically from Singapore. The collective literature emphasizes the importance of harnessing advanced signal processing, ensemble learning, and fine-tuning strategies to address key challenges such as high species diversity, overlapping acoustic signals, and environmental noise. Although Singapore-specific efforts exist and show promising results, there remains a notable gap in published research dedicated solely to Singaporean bird song classification systems. Addressing this gap through continued development of localized data repositories and the adaptation of semi-supervised and transfer learning methods will enhance the accuracy and usability of these systems for biodiversity monitoring in tropical urban and forest contexts.

This review confirms that, indeed, there is published research concerning Singapore's bird sound classification ‚Äì most notably the contributions by Hexeberg et al. (2025) ‚Äì while many global models described in the literature provide methodologies that can be further extended and fine-tuned using local Singapore data. Continued efforts in integrating local recordings with established global models will ultimately lead to more accurate and region-sensitive bioacoustic monitoring systems in Southeast Asia.

## References

Ansar, W., Chatterjee, A., Goswami, S., & Chakrabarti, A. (2024). An efficientnet-based ensemble for bird-call recognition with enhanced noise reduction. *SN Computer Science, 5*, 265. https://doi.org/10.1007/s42979-023-02591-6

Bellafkir, H., Vogelbacher, M., Schneider, D., Kizik, V., M√ºhling, M., & Freisleben, B. (2023). Bird species recognition in soundscapes with self-supervised pre-training. *Communications in Computer and Information Science*, 60-74. https://doi.org/10.1007/978-3-031-46338-9_5

Caprioli, E. (2022). *A semi-supervised approach to bird song classification* [Master's thesis, Norwegian University of Science and Technology]. NTNU Open. https://hdl.handle.net/11250/3093609

Das, N., Padhy, N., Dey, N., Bhattacharya, S., & Tavares, J. M. R. S. (2023). Deep transfer learning-based automated identification of bird song. *International Journal of Interactive Multimedia and Artificial Intelligence, 8*, 33. https://doi.org/10.9781/ijimai.2023.01.003

Hexeberg, S., Chitre, M., Hoffmann‚ÄêKuhnt, M., & Low, B. W. (2025). Semi-supervised classification of bird vocalizations. *ArXiv*. https://doi.org/10.48550/arxiv.2502.13440

Jamil, N., Norali, A. N., Ramli, M. I., Shah, A. K. M. K., & Mamat, I. (2023). Siulmalaya: an annotated bird audio dataset of malaysia lowland forest birds for passive acoustic monitoring. *Bulletin of Electrical Engineering and Informatics, 12*, 2269-2281. https://doi.org/10.11591/beei.v12i4.5243

Zhong, M., Taylor, R., Bates, N., Christey, D., Basnet, H., Flippin, J., Palkovitz, S., Dodhia, R., & Ferres, J. L. (2021). Acoustic detection of regionally rare bird species through deep convolutional neural networks. *Ecological Informatics, 64*, 101333. https://doi.org/10.1016/j.ecoinf.2021.10133311:T8247,
## I. Introduction  
Monitoring avian biodiversity in regions such as Singapore and other Southeast Asian biodiversity hotspots is a fundamental yet challenging task, particularly when relying on acoustic data. Bird sound classification models are critical for passive acoustic monitoring (PAM) as they provide cost‚Äêeffective, scalable, and non‚Äêinvasive means to assess ecosystem health. However, the development of accurate audio‚Äêbased species classifiers is impeded by a paucity of high-quality labeled regional recordings, the occurrence of sensitive and endangered species with restricted datasets, and high species diversity coupled with acoustic similarity among coexisting birds. This literature review investigates machine learning methods applied to train bird sound classification models in data-sparse regional contexts. It places special emphasis on the challenges encountered in regions such as Singapore and explores approaches that mitigate data sparsity by leveraging transfer learning, semi-supervised or weakly supervised techniques, data augmentation, and region-specific fine-tuning of models like BirdNET ([Bellafkir et al., 2023](https://doi.org/10.1007/978-3-031-46338-9_5), [Jamil et al., 2023](https://doi.org/10.11591/beei.v12i4.5243)).

## II. Machine Learning Methods for Bird Sound Classification  
A variety of machine learning paradigms have been adopted for bird sound classification, with deep learning frameworks emerging as the prevailing approach due to their ability to learn complex, non-linear representations from spectrogram images of audio recordings.

### A. Convolutional Neural Networks (CNNs)  
Deep convolutional neural networks (CNNs) are among the most widely used architectures for bird sound classification. CNNs have been successfully applied to spectrogram representations, which capture both temporal and frequency domain features of bird calls. The use of CNNs on spectrograms is fueled by their capacity to automatically learn robust features, thus reducing the need for manual feature engineering ([Stowell et al., 2019](https://doi.org/10.1111/2041-210x.13103)). Advanced CNN architectures, including variants such as AlexNet, VGG16, ResNet50, and DenseNet, have been evaluated for their performance on both large-scale and limited regional datasets. For instance, methods leveraging deep CNNs with attention mechanisms, as seen in some of the recent studies, have demonstrated efficacy in fine-grained bird call classification despite the challenges posed by weak labels and environmental noise ([Bellafkir et al., 2023](https://doi.org/10.1007/978-3-031-46338-9_5)).

### B. Transfer Learning from Global Models  
Transfer learning is a critical strategy to address data scarcity by repurposing feature extractors pre-trained on large datasets (often from domains such as image classification [e.g., the ImageNet dataset]) for bird sound classification tasks. By using transfer learning, models can leverage general acoustic feature representations and effectively adapt to regional datasets with minimal labeled data ([Das et al., 2023](https://doi.org/10.9781/ijimai.2023.01.003)). The BirdNET framework exemplifies this approach by fine-tuning pre-trained CNN models on global bird sound datasets and subsequently adapting them to local recordings, resulting in improved classification accuracy even when training data is limited ([Kahl et al., 2021](https://doi.org/10.1016/j.ecoinf.2021.101236), [Zhong et al., 2021](https://doi.org/10.1016/j.ecoinf.2021.101333)).

### C. Semi-Supervised and Weakly-Supervised Learning  
In regions with sparse labeled data, semi-supervised and weakly-supervised learning approaches have emerged as viable alternatives. These techniques utilize a mixture of labeled and abundant unlabeled audio data to create robust classifiers. For example, FixMatch, a semi-supervised learning algorithm, has been applied to bird sound classification by generating pseudo-labels on unlabeled recordings and improving performance when only a small portion of the dataset is annotated ([Caprioli, 2022](#cite-caprioli2022asemisupervisedapproach)). Likewise, methods based on weak supervision leverage incomplete or imprecise labels by incorporating expert knowledge to fine-tune the model and to mitigate the noise and inconsistencies that often arise in crowdsourced audio recordings ([Conde et al., 2021](https://doi.org/10.48550/arxiv.2107.04878)).

### D. Data Augmentation Strategies  
Data augmentation plays a crucial role in combating overfitting and enhancing the generalization capabilities of classifiers trained on limited regional data. Techniques such as time and pitch shifting, spectrogram axis shifting, mixup, and the addition of background noise (including, for instance, external bird audio recordings) have been applied to artificially expand the dataset. These augmentations help simulate variations encountered in real-world recordings, particularly in acoustically complex environments typical of tropical regions ([Ansar et al., 2024](https://doi.org/10.1007/s42979-023-02591-6), [Nshimiyimana, 2024](https://doi.org/10.1007/s11042-023-17959-2)). By generating synthetic variations, data augmentation also alleviates the challenges of class imbalance, which is especially pertinent when rare species are represented by only a few samples ([Das et al., 2023](https://doi.org/10.9781/ijimai.2023.01.003)).

## III. Challenges in Data-Sparse Regional Contexts  
Training bird sound classification models in regions such as Singapore presents unique challenges that can be grouped into three main areas: limited availability of high-quality labeled audio data, the sensitive nature of datasets concerning endangered species, and the high acoustic similarity amid diverse species.

### A. Limited Availability of High-Quality Labeled Audio Data  
One of the predominant challenges in data-sparse regional contexts is the scarcity of high-quality labeled audio recordings. In many biodiversity hotspots, the production of expert-verified annotated datasets is both time-consuming and resource-intensive. For example, studies like SiulMalaya have addressed these challenges by combining citizen science data with expert annotations, but even then, classification accuracies remain modest due to the limited amount of available training data ([Jamil et al., 2023](https://doi.org/10.11591/beei.v12i4.5243)). Limited datasets force researchers to work with sparse examples, and when these audio recordings are combined with diverse environmental noises, the task of effective bird sound classification becomes significantly more complex.

### B. Sensitive or Endangered Species with Restricted Datasets  
The conservation status of many avian species necessitates careful handling of the related acoustic data. Some rare or endangered species appear infrequently in recordings, resulting in severely imbalanced datasets. This imbalance not only raises issues of overfitting during model training but also increases the likelihood of false negatives, which could impede conservation efforts. As many of these species are of high ecological significance, models must be carefully designed to maintain sensitivity to rare calls while avoiding misclassifications caused by background noise or overlapping vocalizations ([Zhong et al., 2021](https://doi.org/10.1016/j.ecoinf.2021.101333), [Caprioli, 2022](#cite-caprioli2022asemisupervisedapproach)).

### C. High Species Diversity and Acoustic Similarity  
Southeast Asian ecosystems, particularly in regions like Singapore, are characterized by high species diversity. This diversity is accompanied by significant acoustic similarity among species, especially those that co-occur in dense habitats such as urban parks and rainforests. The overlap in frequency ranges and temporal patterns among bird calls increases the complexity of classification, necessitating models capable of discerning subtle differences. In these contexts, even minor variations introduced by different recording conditions or the presence of ambient noise can lead to misclassifications, further complicating the task ([Bellafkir et al., 2023](https://doi.org/10.1007/978-3-031-46338-9_5), [Tang et al., 2024](https://doi.org/10.1371/journal.pone.0297988)).

## IV. Approaches to Mitigate Data Sparsity  
Researchers have developed several strategies to mitigate the inherent difficulties in training accurate bird sound classifiers using limited regional data. These strategies leverage advancements in transfer learning, semi-supervised learning, and data augmentation, among other techniques.

### A. Transfer Learning from Global to Regional Domains  
The concept of transfer learning involves the adaptation of models pre-trained on large global datasets to the specific conditions found in a regional context. For bird sound classification, large-scale datasets such as those collected via Xeno-canto or the BirdCLEF challenges serve as a robust foundation from which models can learn general acoustic features. These models can then be fine-tuned with available regional data to capture local environmental and species-specific characteristics. For instance, models such as ResNet50 and EfficientNet have been successfully adapted from global datasets to recognize calls in regionally limited contexts ([Das et al., 2023](https://doi.org/10.9781/ijimai.2023.01.003), [Kahl et al., 2021](https://doi.org/10.1016/j.ecoinf.2021.101236)). BirdNET further exemplifies this strategy by employing region-specific fine-tuning that incorporates quality-based loss weighting and threshold calibration to adapt to the local acoustic domain, thereby improving performance on limited and noisy regional audio samples ([Bellafkir et al., 2023](https://doi.org/10.1007/978-3-031-46338-9_5), [Ansar et al., 2024](https://doi.org/10.1007/s42979-023-02591-6)).

### B. Semi-Supervised, Self-Supervised, and Weakly-Supervised Learning Techniques  
When labeled data are sparse, semi-supervised and weakly-supervised methods allow models to leverage the abundance of unlabeled recordings. Semi-supervised algorithms, such as FixMatch, combine a small, high-quality labeled dataset with a larger pool of unlabeled data by generating pseudo-labels and filtering them using confidence thresholds. This approach has been shown to improve accuracy by effectively expanding the training data without incurring the high costs of manual annotation ([Caprioli, 2022](#cite-caprioli2022asemisupervisedapproach)). Similarly, weakly supervised techniques address label noise by working with incomplete or imprecise labels, adapting to the inherent uncertainty in field-collected recordings, and thereby enabling the training of robust classifiers even with limited labeled data ([Conde et al., 2021](https://doi.org/10.48550/arxiv.2107.04878)).

### C. Data Augmentation and Synthetic Data Generation  
Data augmentation strategies serve as another cornerstone for mitigating the challenges associated with limited regional datasets. By applying transformations such as time stretching, pitch shifting, adding synthetic background noise, and mixup methods, researchers can artificially expand the available dataset and introduce variability that helps the model generalize better to unseen data. Such techniques have been particularly effective in overcoming issues related to environmental variability and class imbalance ([Ansar et al., 2024](https://doi.org/10.1007/s42979-023-02591-6), [Nshimiyimana, 2024](https://doi.org/10.1007/s11042-023-17959-2)). Additionally, the generation of synthetic data through proxy species or simulated audio environments can further enrich training datasets, thereby compensating for the scarcity of examples for rare or sensitive species.

### D. Region-Specific Fine-Tuning of Pre-Trained Models  
An effective way to bridge the gap between global models and local conditions is through region-specific fine-tuning. Models that are initially pre-trained on large and diverse datasets capture general acoustic patterns that are then refined using limited regional recordings. Fine-tuning of models such as BirdNET on localized data allows the classifier to account for differences in environmental acoustics, species behavior, and recording equipment. This process not only improves recognition accuracy but also helps in adjusting model sensitivity to variations that are specific to regions like Singapore ([Zhong et al., 2021](https://doi.org/10.1016/j.ecoinf.2021.101333), [Rajan & Noumida, 2021](https://doi.org/10.1109/iccisc52257.2021.9484858)).

### E. Integration of Meta Information and Proxy Modalities  
In addition to traditional acoustic features derived from spectrograms, incorporating auxiliary meta information‚Äîsuch as textual descriptions of bird calls, ecological traits, and life-history data‚Äîcan further bolster classification performance. Meta-information allows models to contextualize audio data by leveraging additional cues about species' habitats, morphology, and behavior. Recent studies have shown that concatenating AVONET features (including ecological and morphological traits) with life-history characteristics can improve zero-shot audio classification performance, thereby enhancing the model's ability to generalize from global datasets to region-specific contexts ([Gebhard et al., 2024](https://doi.org/10.1109/icassp48485.2024.10445807)).

## V. Synthesis of Approaches for Southeast Asia and Singapore  
The convergence of methods such as transfer learning, semi-supervised techniques, and robust data augmentation offers a promising path forward for developing accurate bird sound classifiers in data-sparse regional contexts, particularly in Southeast Asia and Singapore. In practice, these methods can be integrated into a comprehensive pipeline that begins with the acquisition and pre-processing of raw audio data followed by the application of advanced CNN architectures. Pre-processing steps include cropping long recordings into manageable time windows, converting the audio into Mel spectrograms, and applying noise reduction filters tailored to the local acoustic environment ([Bellafkir et al., 2023](https://doi.org/10.1007/978-3-031-46338-9_5), [LeBien et al., 2020](https://doi.org/10.1016/j.ecoinf.2020.101113)).

Initial training on large-scale global datasets enables the model to learn generic acoustic features, which are then transferred to the target domain through fine-tuning with regional audio. This stage is critical in regions such as Singapore where environmental conditions, recording equipment quality, and species vocalizations vary significantly from those found in global datasets ([Das et al., 2023](https://doi.org/10.9781/ijimai.2023.01.003), [Kahl et al., 2021](https://doi.org/10.1016/j.ecoinf.2021.101236)). Following the transfer learning stage, semi-supervised and weakly supervised techniques further expand the training dataset through pseudo-labeling of unlabeled recordings, thereby mitigating the effects of scarce annotated data ([Caprioli, 2022](#cite-caprioli2022asemisupervisedapproach), [Conde et al., 2021](https://doi.org/10.48550/arxiv.2107.04878)).

Data augmentation remains an integral enhancement in such pipelines, where techniques ranging from simple time-pitch shifting to complex mixup training are utilized to simulate the variability of natural soundscapes. This is particularly important for training robust classifiers capable of distinguishing between acoustically similar species in biodiverse regions ([Ansar et al., 2024](https://doi.org/10.1007/s42979-023-02591-6), [Nshimiyimana, 2024](https://doi.org/10.1007/s11042-023-17959-2)). Moreover, integrating meta-information associated with the birds, such as ecological attributes and life-history traits, can offer an additional layer of context. This approach is crucial when distinguishing between species with high acoustic similarity, ensuring that the classifier accounts for subtle yet biologically meaningful differences ([Gebhard et al., 2024](https://doi.org/10.1109/icassp48485.2024.10445807)).

In scenarios where the acoustic recordings include data for rare or endangered species, methods such as proxy species generation and the use of synthetic data can further compensate for the limited number of examples available. Not only do these methods enrich the overall dataset, but they also help in training models that are resilient to the variances in call frequency and intensity found among rare species ([Rajan & Noumida, 2021](https://doi.org/10.1109/iccisc52257.2021.9484858), [Stowell et al., 2019](https://doi.org/10.1111/2041-210x.13103)).

## VI. Discussion and Future Directions  
Drawing from the diverse approaches detailed in the reviewed literature, it is evident that the combination of modern deep learning techniques with domain-specific adaptations is key to overcoming data sparsity in bird sound classification tasks. The effectiveness of transfer learning is particularly notable‚Äîpre-trained models that are fine-tuned with regional data can bridge the gap between heterogeneous audio domains and provide high classification accuracy despite sparse labeled examples ([Das et al., 2023](https://doi.org/10.9781/ijimai.2023.01.003), [Kahl et al., 2021](https://doi.org/10.1016/j.ecoinf.2021.101236)).

Moving forward, several research directions appear promising. One area of active research is the further development of self-supervised learning algorithms that do not require any annotated labels at all during pre-training. With the advent of self-supervised models in other domains, there is potential to harness large volumes of unlabeled audio recordings from biodiversity hotspots, thus reducing reliance on manual annotations ([Caprioli, 2022](#cite-caprioli2022asemisupervisedapproach)). Researchers might also explore architectures that combine CNN and recurrent neural network (RNN) layers to capture both spectral and temporal features more effectively, as these hybrid models have demonstrated improved performance in recognizing overlapping and sequential bird vocalizations ([Stowell et al., 2019](https://doi.org/10.1111/2041-210x.13103)).

Furthermore, the integration of active learning strategies‚Äîwhere the model selectively queries the most uncertain examples for expert labeling‚Äîcould help optimize the annotation process in data-sparse environments. Active learning has the potential to greatly reduce the labeling effort required, ensuring that only the most impactful data points are annotated, thereby enhancing overall classifier performance ([Clink et al., 2024](https://doi.org/10.1101/2024.08.17.608420)).

Region-specific studies are essential to validate and refine these methodologies. In the context of Singapore, a densely populated and ecologically complex urban setting, factors such as urban noise, seasonal changes, and the influence of diverse habitat types must be considered. Future work should involve collecting more localized audio data and implementing fine-tuning steps that emphasize these unique acoustic properties while incorporating community-driven citizen science initiatives to build larger, high-quality annotated datasets ([Jamil et al., 2023](https://doi.org/10.11591/beei.v12i4.5243), [LeBien et al., 2020](https://doi.org/10.1016/j.ecoinf.2020.101113)).

Another promising avenue is the use of ensemble modeling techniques, where multiple classifiers are combined to improve overall accuracy and robustness. Ensemble approaches have been shown to reduce the variance inherent in single-model predictions, thereby yielding more reliable results in challenging acoustic environments ([Henkel & Singer, 2021](https://doi.org/10.48550/arxiv.2107.07728), [Rajan & Noumida, 2021](https://doi.org/10.1109/iccisc52257.2021.9484858)).

Integration of multi-modal data also presents a strong opportunity. For instance, coupling audio data with environmental and visual metadata could provide additional discriminative power. This multi-modal approach is particularly relevant for habitats where bird calls are acoustically similar. By aligning acoustic signals with spatial, temporal, and ecological metadata, classifiers could achieve improved differentiation among species that are otherwise challenging to distinguish based solely on audio ([Gebhard et al., 2024](https://doi.org/10.1109/icassp48485.2024.10445807), [Tang et al., 2024](https://doi.org/10.1371/journal.pone.0297988)).

## VII. Recommendations for Implementation in Singapore and Similar Southeast Asian Contexts  
Based on the literature reviewed, researchers and practitioners working in data-sparse regional contexts such as Singapore should consider the following recommendations for developing robust bird sound classification systems:

### 1. Leverage Global Transfer Learning:  
   Initiate model training on extensive, established global bird acoustic databases and fine-tune on a curated subset of local recordings. Pre-trained CNN architectures, especially those adapted into frameworks such as BirdNET, should form the backbone of regional classifiers due to their demonstrated ability to generalize across diverse acoustic domains ([Das et al., 2023](https://doi.org/10.9781/ijimai.2023.01.003), [Kahl et al., 2021](https://doi.org/10.1016/j.ecoinf.2021.101236)).

### 2. Incorporate Semi- and Weakly-Supervised Techniques:  
   Exploit the abundance of unlabeled audio data available via passive acoustic monitoring networks by adopting semi-supervised learning methods such as FixMatch or alternative weakly supervised algorithms. This approach will help bridge the gap caused by limited expert annotations, enabling models to learn from both high-quality labeled recordings and a larger pool of unlabeled data ([Caprioli, 2022](#cite-caprioli2022asemisupervisedapproach), [Conde et al., 2021](https://doi.org/10.48550/arxiv.2107.04878)).

### 3. Employ Robust Data Augmentation:  
   Use a comprehensive suite of data augmentation techniques to enhance training dataset diversity. Time and pitch shifting, noise injection, and mixup training can simulate various recording conditions encountered in urban and forested areas in Singapore, improving the model's ability to generalize under varying environmental conditions ([Ansar et al., 2024](https://doi.org/10.1007/s42979-023-02591-6), [Nshimiyimana, 2024](https://doi.org/10.1007/s11042-023-17959-2)).

### 4. Focus on Region-Specific Fine-Tuning:  
   After transfer learning from global datasets, dedicate resources to fine-tuning the model on locally collected data. Incorporate region-specific acoustic characteristics and environmental factors into the training process. This step is crucial in ensuring that the model remains sensitive to the subtle inter-species variations and local noise conditions characteristic of Southeast Asian soundscapes ([Zhong et al., 2021](https://doi.org/10.1016/j.ecoinf.2021.101333), [Rajan & Noumida, 2021](https://doi.org/10.1109/iccisc52257.2021.9484858)).

### 5. Integrate Meta Information and Multi-Modal Data:  
   Supplement audio features with relevant meta information such as species morphological traits, ecological data, and temporal recording metadata. This integrative approach can help disambiguate calls from species with high acoustic similarity, enabling more precise classification in biodiversity-rich regions where multiple, overlapping signals are common ([Gebhard et al., 2024](https://doi.org/10.1109/icassp48485.2024.10445807), [Tang et al., 2024](https://doi.org/10.1371/journal.pone.0297988)).

### 6. Utilize Ensemble Approaches and Active Learning:  
   Combine multiple machine learning models to form an ensemble that reduces variance and enhances the robustness of predictions. Additionally, implement active learning strategies to prioritize labeling of the most ambiguous recordings, thereby optimizing the use of limited expert resources ([Henkel & Singer, 2021](https://doi.org/10.48550/arxiv.2107.07728), [Clink et al., 2024](https://doi.org/10.1101/2024.08.17.608420)).

## VIII. Conclusion  
The literature reviewed herein clearly demonstrates that modern deep learning techniques‚Äîaugmented by transfer learning, semi-supervised methods, and robust data augmentation‚Äîoffer a promising solution to the problem of training bird sound classification models in data-sparse regional contexts. Regions like Singapore, characterized by high biodiversity and complex, acoustically noisy environments, present unique challenges that can be effectively addressed through the integration of global models with region-specific fine-tuning, active learning, and meta-data fusion. While the scarcity of high-quality labeled data remains a significant obstacle, the combination of these approaches promises to enhance monitoring capabilities, support conservation efforts for endangered species, and ultimately contribute to a more informed understanding of Southeast Asian biodiversity ([Bellafkir et al., 2023](https://doi.org/10.1007/978-3-031-46338-9_5), [Das et al., 2023](https://doi.org/10.9781/ijimai.2023.01.003), [Jamil et al., 2023](https://doi.org/10.11591/beei.v12i4.5243), [Stowell et al., 2019](https://doi.org/10.1111/2041-210x.13103)).

Future research is encouraged to expand on these methodologies by incorporating emerging techniques such as self-supervised representation learning and the further exploration of multi-modal data fusion. Such advancements are essential for the evolution of PAM systems that can overcome the inherent limitations of data-sparse environments. Researchers should also prioritize the creation of large-scale, expert-annotated regional datasets‚Äîpotentially through collaborations that combine citizen science efforts with standardized recording protocols‚Äîto further refine model performance and generalization in tropical urban and forest settings ([LeBien et al., 2020](https://doi.org/10.1016/j.ecoinf.2020.101113), [Tang et al., 2024](https://doi.org/10.1371/journal.pone.0297988), [Zhong et al., 2021](https://doi.org/10.1016/j.ecoinf.2021.101333)).

In summary, the integration of transfer learning, semi-supervised learning, data augmentation, and region-specific fine-tuning constitutes the cornerstone of effective bird sound classification in Southeast Asia. This approach not only addresses the challenge of limited annotated data but also facilitates the recognition of highly similar and overlapping bird calls in complex soundscapes, ultimately contributing to robust and scalable biodiversity monitoring systems. Such systems are essential for informing conservation strategies and ensuring that even rare or endangered species are appropriately monitored, thereby supporting the broader goals of ecosystem management and biodiversity conservation ([Das et al., 2023](https://doi.org/10.9781/ijimai.2023.01.003), [Kahl et al., 2021](https://doi.org/10.1016/j.ecoinf.2021.101236), [Rajan & Noumida, 2021](https://doi.org/10.1109/iccisc52257.2021.9484858)).

By adopting and further refining these strategies, practitioners in regions like Singapore can transform sparse audio datasets into valuable, actionable insights that drive the next generation of environmental monitoring and conservation efforts. This body of work, drawing on advances from both global and region-specific studies, offers a comprehensive framework for overcoming data limitations and achieving high-performance bird sound classification in challenging ecological contexts.

In conclusion, accurate bird sound classification in data-sparse regional environments is not only feasible but also essential for effective biodiversity monitoring. The successful integration of advanced machine learning techniques with contextual domain knowledge ensures that even in regions with limited data, reliable and scalable models can be developed. Researchers must continue to explore and refine these integrated approaches, as they hold the key to bridging the gap between global methodologies and local environmental challenges, ultimately fostering a deeper understanding of the rich and diverse avifauna in Southeast Asia ([Bellafkir et al., 2023](https://doi.org/10.1007/978-3-031-46338-9_5), [Ansar et al., 2024](https://doi.org/10.1007/s42979-023-02591-6), [Stowell et al., 2019](https://doi.org/10.1111/2041-210x.13103)).

Through collaborative efforts that combine expertise in machine learning, ecology, and local field studies, the future of bird sound classification in biodiversity hotspots is bright. Such collaborative initiatives will pave the way for more effective deployment of PAM systems, ensuring that limited regional datasets are leveraged to their fullest potential, thereby contributing significantly to conservation and ecological research in dynamic and complex urban and natural environments.

## References

1. Ansar, W., Chatterjee, A., Goswami, S., & Chakrabarti, A. (2024). [An EfficientNet-based ensemble for bird-call recognition with enhanced noise reduction](https://doi.org/10.1007/s42979-023-02591-6). *SN Computer Science, 5*, 265. (4 citations, peer-reviewed)

2. Bellafkir, H., Vogelbacher, M., Schneider, D., Kizik, V., M√ºhling, M., & Freisleben, B. (2023). [Bird species recognition in soundscapes with self-supervised pre-training](https://doi.org/10.1007/978-3-031-46338-9_5). *Communications in Computer and Information Science*, 60-74. (3 citations, peer-reviewed)

3. Caprioli, E. (2022). [A semi-supervised approach to bird song classification](https://hdl.handle.net/11250/3093609). Master's thesis, Norwegian University of Science and Technology (NTNU). Advisor: Downing, K.

4. Clink, D. J., Cross-Jaya, H., Kim, J., Ahmad, A. H., Hong, M., Sala, R., Birot, H., Agger, C., Vu, T. T., Thi, H. N., Chi, T. N., & Klinck, H. (2024). [Benchmarking automated detection and classification approaches for monitoring of endangered species: a case study on gibbons from Cambodia](https://doi.org/10.1101/2024.08.17.608420). *bioRxiv*. (0 citations)

5. Conde, M. V., Shubham, K., & Agnihotri, P. (2021). [Weakly-supervised classification and detection of bird sounds in the wild: a BirdCLEF 2021 solution](https://doi.org/10.48550/arxiv.2107.04878). *ArXiv*.

6. Das, N., Padhy, N., Dey, N., Bhattacharya, S., & Tavares, J. M. R. S. (2023). [Deep transfer learning-based automated identification of bird song](https://doi.org/10.9781/ijimai.2023.01.003). *International Journal of Interactive Multimedia and Artificial Intelligence, 8*, 33. (3 citations)

7. Gebhard, A., Triantafyllopoulos, A., Bez, T., Christ, L., Kathan, A., & Schuller, B. W. (2024). [Exploring meta information for audio-based zero-shot bird classification](https://doi.org/10.1109/icassp48485.2024.10445807). *ICASSP 2024 - IEEE International Conference on Acoustics, Speech and Signal Processing*, 1211-1215. (6 citations)

8. Henkel, C., & Singer, P. (2021). [Recognizing bird species in diverse soundscapes under weak supervision](https://doi.org/10.48550/arxiv.2107.07728). *ArXiv*.

9. Jamil, N., Norali, A. N., Ramli, M. I., Shah, A. K. M. K., & Mamat, I. (2023). [Siulmalaya: an annotated bird audio dataset of Malaysia lowland forest birds for passive acoustic monitoring](https://doi.org/10.11591/beei.v12i4.5243). *Bulletin of Electrical Engineering and Informatics, 12*, 2269-2281. (3 citations)

10. Kahl, S., Wood, C. M., Eibl, M., & Klinck, H. (2021). [BirdNET: A deep learning solution for avian diversity monitoring](https://doi.org/10.1016/j.ecoinf.2021.101236). *Ecological Informatics, 61*, 101236. (648 citations, peer-reviewed)

11. LeBien, J., Zhong, M., Campos-Cerqueira, M., Velev, J. P., Dodhia, R., Ferres, J. L., & Aide, T. M. (2020). [A pipeline for identification of bird and frog species in tropical soundscape recordings using a convolutional neural network](https://doi.org/10.1016/j.ecoinf.2020.101113). *Ecological Informatics, 59*, 101113. (161 citations, peer-reviewed)

12. Nshimiyimana, A. (2024). [Acoustic data augmentation for small passive acoustic monitoring datasets](https://doi.org/10.1007/s11042-023-17959-2). *Multimedia Tools and Applications, 83*, 63397-63415. (3 citations, peer-reviewed)

13. Rajan, R., & Noumida, A. (2021). [Multi-label bird species classification using transfer learning](https://doi.org/10.1109/iccisc52257.2021.9484858). *International Conference on Communication, Control and Information Sciences (ICCISc)*, 1-5. (23 citations)

14. Stowell, D., Wood, M. D., Pamu≈Ça, H., Stylianou, Y., & Glotin, H. (2019). [Automatic acoustic detection of birds through deep learning: the first bird audio detection challenge](https://doi.org/10.1111/2041-210x.13103). *Methods in Ecology and Evolution, 10*, 368-380. (420 citations, highest quality peer-reviewed journal)

15. Tang, Y., Liu, C., & Yuan, X. (2024). [Recognition of bird species with birdsong records using machine learning methods](https://doi.org/10.1371/journal.pone.0297988). *PLOS ONE, 19*, e0297988. (4 citations, peer-reviewed)

16. Zhong, M., Taylor, R., Bates, N., Christey, D., Basnet, H., Flippin, J., Palkovitz, S., Dodhia, R., & Ferres, J. L. (2021). [Acoustic detection of regionally rare bird species through deep convolutional neural networks](https://doi.org/10.1016/j.ecoinf.2021.101333). *Ecological Informatics, 64*, 101333. (49 citations, peer-reviewed)







12:T2f69,
### [BirdNET](https://birdnet.cornell.edu/)

#### Overview

BirdNET is a free AI-powered bird sound identification tool developed by the Cornell Lab of Ornithology and Chemnitz University. It is very user-friendly ‚Äì no birding experience is needed; users simply record a bird‚Äôs song on their smartphone and let the app identify it. BirdNET can recognize over 3,000 bird species by sound (as of 2022) and is continually expanding its coverage. The app is available for iOS and Android devices, and there‚Äôs also a web demo for uploading audio clips.

#### Ease of Use
BirdNET‚Äôs interface is simple: just tap Record to capture a bird call, then tap Analyze. The recording is uploaded to BirdNET‚Äôs servers, which quickly return the most likely species matches. Results include a probability score (e.g. ‚ÄúHighly Certain‚Äù or ‚ÄúUncertain‚Äù) for each identification. Because analysis happens in the cloud, an internet connection is required during use. Overall, the app lowers the barrier for birding by ear ‚Äì even total beginners can get an ID by just using their phone‚Äôs microphone. (Note: BirdNET is a citizen-science tool; each confirmed recording can be submitted to researchers for conservation studies.)


#### Access

 Download BirdNET from the App Store or Google Play (free). Open the app and allow microphone and location access. When you hear a bird, press the record button and wait a few seconds to capture the sound. Next, select the portion of the spectrogram (visual representation of the sound) that contains the bird call and tap the Identify button. The app will display the top species matches for the sound, along with confidence ratings. You can tap a result to learn more about the bird. It‚Äôs that easy ‚Äì BirdNET handles the complex audio analysis using its trained neural network on the back-end.

### [Merlin](https://merlin.allaboutbirds.org/)

#### Overview

Merlin Bird ID is a popular bird identification app from Cornell Lab, geared toward the general public. Initially known for photo and sighting IDs, Merlin added a Sound ID feature in 2021 that can recognize hundreds of species by their songs and calls. It‚Äôs also free and available on iOS and Android. Merlin is designed to be extremely easy to use, making birding by ear feel ‚Äúlike magic‚Äù for beginners.

#### Ease of Use

Using Merlin‚Äôs Sound ID is straightforward and works offline once you‚Äôve downloaded a region-specific bird pack. As the app listens through your phone‚Äôs microphone, it uses AI to analyze the sound and in real time displays the names and photos of species it hears. The live feedback means you can watch a list of bird names appear as each bird sings, which is very intuitive. Merlin can even detect multiple birds at once and highlight each species when it sings, helping users parse bird choruses. This real-time, continuous identification makes Merlin a great learning tool ‚Äì users can tap on an identified bird to see more info (calls, range, ID tips), or replay the recording to reinforce recognition.

#### Access & Usage

Install Merlin Bird ID from your app store (free). Upon first use, you‚Äôll download a bird pack for your region (which includes sounds for the local species). To use Sound ID, open the app and tap Sound ID. Simply hold up your phone and tap the microphone icon to start listening. Merlin will continuously display any species it recognizes. You can tap a species name in the list to pinpoint where in the recording that bird sang, play back that segment, and read about the species. Merlin‚Äôs sound identification works without a data connection (ideal for remote areas) since the recognition runs on your device.

### [Song Sleuth](https://www.sibleyguides.com/product/song-sleuth/) 

#### Overview

Song Sleuth is a smartphone app (iOS; a past Android version was planned) that was one of the first bird song recognition tools for North America. Developed by Wildlife Acoustics with birding expert David Sibley, the app can identify about 200 common bird species by their songs. It‚Äôs a paid app (around $10) and is geared toward bird enthusiasts who want more than a quick ID ‚Äì it also provides illustrations, species info, and a spectrogram visualization for learning purposes.

#### Ease of Use

Song Sleuth‚Äôs interface is a bit more technical but rewarding. When you open the app, it continuously displays a live spectrogram (graph of sound frequencies) of all sounds around you. To ID a bird, you press record when the bird sings; the app automatically captures the song (even a few seconds prior to tapping, using a buffer) and highlights it on the spectrogram. Then tap the Identify button, and Song Sleuth will analyze the sound and present a short list of possible species matches. In tests, it often provides the correct species or at least gets you ‚Äúin the ballpark‚Äù of similar sounding birds. This semi-manual process (selecting the target call on the spectrogram) means there is a small learning curve, but it was praised as an elegant system for users to visualize and identify songs. The app also saves your recordings and identifications for later review or study.

#### Access & Usage

Song Sleuth is available on the Apple App Store (no longer officially supported, but still downloadable). After installing, launch the app and choose your region if prompted (to filter species). When birding, open Song Sleuth and watch the scrolling spectrogram. When you hear a bird, tap Record to capture the sound (then Stop). The app will mark the suspected bird song automatically; if needed, adjust the selection on the spectrogram. Next, tap Analyze to get identification results. You‚Äôll see a list of likely species (often with the top 2‚Äì3 candidates) and you can compare your recording to example songs in the app‚Äôs library. Remember that Song Sleuth doesn‚Äôt require internet ‚Äì all processing is on-device. Note: The developer is no longer updating Song Sleuth (as of 2021), so its species list is fixed at 200 and future OS compatibility isn‚Äôt guaranteed.

### [ChirpOMatic UK](https://www.chirpomatic.com/) 

#### Overview 

ChirpOMatic is another widely available birdsong recognition app, originally developed for the U.K. and now with a North American version as well. It‚Äôs a paid mobile app (available on iOS, and on Android in some regions) focused on simplicity for casual users. ChirpOMatic contains a library of common bird sounds and uses AI to match your recording to these sounds. Its species coverage is more regional (e.g. a UK app version for British birds, and a separate app for North America).

#### Ease of Use

ChirpOMatic‚Äôs workflow is very straightforward: you open the app and manually start a recording when you hear a bird. (It doesn‚Äôt continuously buffer audio like some others, so you do have to anticipate the bird‚Äôs song.) After recording a clip of the bird, you tap Identify, and the app will list the likely species. In reviewer tests, ChirpOMatic‚Äôs accuracy was comparable to Song Sleuth for clear recordings. The app includes a built-in library of bird calls and even lets you save and submit your recordings to help improve the AI model over time. This community feedback approach means the app can get better with user contributions. The interface is user-friendly, with minimal buttons ‚Äì essentially record, stop, and results ‚Äì making it approachable for beginners.

#### Access & Usage

Download ChirpOMatic UK or ChirpOMatic USA from the app store (they are separate apps tailored to regional birds). In the field, launch the app and press the Record button as soon as the bird starts singing. Try to get a few seconds of clear audio, then press Stop. Tap the Identify (or ‚ÄúAnalyze‚Äù) option, and the app will compare your clip to its bird sound database. It will show the best match or a short list of possible species. You can tap a result to confirm by listening to a reference recording of that species. The app doesn‚Äôt require an internet connection for identification, which is handy in remote areas. Finally, if you want, use the option to Submit the recording ‚Äì this uploads your clip for the developers to potentially use in refining their AI (making ChirpOMatic a kind of citizen-science helper as well).

### [BirdNET-Analyzer](https://github.com/kahst/BirdNET-Analyzer)

#### Overview

BirdNET-Analyzer is a research-oriented tool based on the BirdNET AI, intended for bulk analysis of audio recordings rather than real-time identification. While the BirdNET app serves the public, BirdNET-Analyzer is designed for scientists and conservationists dealing with large datasets (for example, months of autonomous recorder files). It‚Äôs essentially the BirdNET neural network packaged with a user-friendly interface for desktop use. The software is free and open-source, available on GitHub for Windows, Linux, and MacOS. (BirdNET‚Äôs functionality has also been integrated into Cornell‚Äôs flagship bioacoustics program, Raven Pro, for researchers who use that software.)

#### Ease of Use

For a research tool, BirdNET-Analyzer is considered easy to install and run. It provides a simple GUI on Windows ‚Äì you don‚Äôt need advanced programming skills to use it. Researchers can load a folder of audio files, choose settings like location/time filters or confidence thresholds, and then let the software automatically identify bird calls in those recordings. The output typically includes timestamps of detections and the predicted species names with confidence scores. This significantly speeds up analysis of passive acoustic monitoring data, where manual review would be too time-consuming. Because it‚Äôs the same core AI as the app, it can detect a wide range of species. In fact, the latest BirdNET-Analyzer release can recognize over 6,500 species of birds worldwide, making it suitable for global research projects. (It has even begun to include other animal sounds like certain frogs and primates in recent updates.)

#### Access & Usage

BirdNET-Analyzer is distributed via GitHub as a free download. To use it, one typically downloads the program (or Python package) and a pre-trained model file. The tool can be run with a graphical interface (on Windows) or via command-line for scripting (useful for large-scale processing on a server). Basic usage involves selecting an input directory of audio files and an output directory, then clicking Run. The software will process each file, scanning for bird sounds and logging any identified species with timing and confidence. You can refine results by adjusting the confidence threshold (to balance between missing faint calls and filtering out false positives). The documentation provides guidelines for these settings, and since it‚Äôs open-source, advanced users can even retrain or tweak the model for specific research needs. 

#### Research Orientation 

This tool is mainly used in scientific studies and conservation projects, not by casual birders. It has been used to validate acoustic monitoring of cryptic bird species in forests, and generally aims to empower researchers to leverage AI for biodiversity surveys. (For example, a biologist could deploy autonomous recorders in the field, then use BirdNET-Analyzer to rapidly identify which bird species vocalized in the recordings each day.) Its strength lies in processing large audio datasets efficiently and consistently ‚Äì a clear win for bioacoustics research.

### References

1. Cornell Lab of Ornithology ‚Äì AI-powered BirdNET app makes citizen science easier (June 28, 2022)
2. BirdNET official website ‚Äì About BirdNET and download links (BirdNET app usage)
3. Cornell Lab of Ornithology ‚Äì What bird is singing? Merlin Bird ID app offers instant answers (June 23, 2021)
4. BirdWatching Magazine ‚Äì The best birdsong apps (2021/2022)
5. Audubon Magazine ‚Äì Testing Out Song Sleuth, a New App That Identifies Birds by Their Calls (Feb 21, 2017)
6. Granjon et al. (2023) ‚Äì Hearing the Unseen: AudioMoth and BirdNET as a Cheap and Easy Method for Monitoring Cryptic Bird Species, Sensors (MDPI) (BirdNET-Analyzer details)

13:T11f3,
### Introduction

Bird vocalizations serve as a cornerstone of avian ecology, playing indispensable roles in behaviors critical for survival and reproduction. However, increasing urbanization has introduced significant anthropogenic noise pollution, severely impeding acoustic communication. This report conducts a comparative analysis of bird vocalization characteristics in urban and forest habitats and explores avian strategies to mitigate noise masking.

### Bird Vocalizations in Forest Habitats

In natural forest environments, bird songs evolve to optimize transmission through dense vegetation. Forests favor lower frequency, tonal vocalizations, and the acoustic environment is shaped by vegetation density, habitat openness, and wildlife presence, guiding song efficiency.

### Bird Vocalizations in Urban Habitats

Urban environments drastically alter bird vocalizations:

- **Song Complexity:** Urban song thrushes show greater syllable repertoire and repetition; song sparrows show no significant difference.
- **Frequency Shift:** Urban species like blackbirds and great tits sing at higher frequencies.
- **Temporal Characteristics:** Urban birds sing longer, faster songs; some shift singing times earlier.
- **Amplitude Modulation:** Urban birds, like blackbirds, sing louder; white-crowned sparrows sang softer during the pandemic.
- **Syllable Usage:** Urban song thrushes favor more twitter syllables over whistle syllables.

### Strategies for Mitigating Anthropogenic Noise

Birds employ:

- **Active Adjustments:** Immediate song modifications (e.g., Lombard effect, changing singing times).
- **Evolutionary Adaptations:** Genetic shifts favoring high-frequency song production.
- **Behavioral Adaptations:** Strategic perch selection and timing adjustments.

### Other Influencing Factors

Urban factors influencing vocalization:

- **Population Density:** Drives longer, faster songs.
- **Urban Structure:** Reflects and distorts sound differently from forests.
- **Morphology:** Vocal tract structures constrain sound production.

### Ecological and Behavioral Implications

- **Mate Choice:** Song changes influence mate selection and reproductive isolation.
- **Territorial Defense:** Noise masks aggressive signals, impairing territory defense.
- **Fitness Costs:** Higher frequencies may reduce vocal performance, impacting fitness.

### Theoretical Context: Acoustic Adaptation Hypothesis (AAH)

AAH posits habitat-specific signal evolution. Urban bird songs partially align with AAH predictions but other pressures like social interactions also play significant roles.

### Conservation Implications

Recommendations for conservation:

- Reduce urban noise.
- Create quieter green spaces.
- Improve habitat connectivity.
- Use bioacoustic monitoring for assessment.

Future research should prioritize the long-term ecological and evolutionary consequences of urbanization on bird vocalizations.

### Conclusion

Urbanization leads to significant alterations in bird vocalizations, mainly due to anthropogenic noise. Birds adapt through frequency shifts, complexity adjustments, and timing modifications, affecting mate choice, territorial defense, and overall fitness. Conservation strategies must aim to mitigate noise and protect avian biodiversity.

### Summary of Observed Changes in Bird Song Characteristics in Urban vs. Forest Habitats

| Species | Song Characteristic | Urban vs. Forest | Potential Driving Factor(s) |
|:-------|:---------------------|:----------------|:----------------------------|
| Song Thrush | Syllable Repertoire Size | Greater | Anthropogenic Noise |
| Song Thrush | Syllable Sequence Repetition | More Frequent | Anthropogenic Noise |
| Song Thrush | Whistle Syllable Proportion | Smaller | Anthropogenic Noise |
| Song Thrush | Twitter Syllable Proportion | Higher | Anthropogenic Noise |
| Song Thrush | Whistle Syllable Frequency | Higher (Min & Peak) | Anthropogenic Noise |
| Blackbird | Minimum Frequency | Higher | Anthropogenic Noise |
| Blackbird | Song Amplitude | Higher | Higher Frequency Usage (Correlation) |
| Northern Cardinal | Minimum Frequency | Higher | Anthropogenic Noise |
| Northern Cardinal | Song Length | Longer | Higher Population Density, Territorial Interactions |
| Northern Cardinal | Song Rate | Faster | Higher Population Density, Territorial Interactions |
| Great Tit | Minimum Frequency | Higher | Anthropogenic Noise |
| Song Sparrow | Repertoire Size | No Significant Difference |  - |
| White-crowned Sparrow | Song Amplitude | Softer (During Pandemic) | Reduced Anthropogenic Noise |

0:{"P":null,"b":"nli5VSm0dNMiC8p1OoAct","p":"","c":["",""],"i":false,"f":[[["",{"children":["__PAGE__",{}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/730b9168ceaa3868.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/_next/static/css/b3cbcd051438d1d5.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","suppressHydrationWarning":true,"children":["$","body",null,{"className":"__className_d65c78","children":["$","$L2",null,{"attribute":"class","defaultTheme":"light","enableSystem":false,"disableTransitionOnChange":true,"children":[["$","$L3",null,{}],["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}],["$","footer",null,{"className":"py-8 border-t","children":["$","div",null,{"className":"container mx-auto px-4","children":[["$","div",null,{"className":"flex flex-col md:flex-row justify-between items-center gap-6","children":[["$","div",null,{"className":"flex flex-col items-center md:items-start","children":[["$","div",null,{"className":"flex items-center gap-2 mb-2","children":[["$","div",null,{"className":"w-8 h-8 rounded-full overflow-hidden mr-2","children":["$","$L6",null,{"src":"/pitta-gpt.png","alt":"SingBirds Logo","width":32,"height":32,"className":"w-full h-full object-cover"}]}],["$","h3",null,{"className":"text-xl font-bold gradient-text","children":"SingBirds"}]]}],["$","p",null,{"className":"text-sm text-muted-foreground text-center md:text-left","children":"I Code Birds."}]]}],["$","div",null,{"className":"flex gap-4","children":["$","a",null,{"href":"https://github.com/fairy-pitta","target":"_blank","rel":"noopener noreferrer","className":"text-muted-foreground hover:text-sky-500 transition-colors","aria-label":"GitHub","children":["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-github h-6 w-6","children":[["$","path","tonef",{"d":"M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"}],["$","path","9comsn",{"d":"M9 18c-4.51 2-5-2-7-2"}],"$undefined"]}]}]}]]}],["$","div",null,{"className":"border-t mt-6 pt-6 flex flex-col md:flex-row justify-between items-center gap-4","children":[["$","div",null,{"className":"text-sm text-muted-foreground","children":["¬© ",2025," SingBirds. All rights reserved."]}],["$","div",null,{"className":"flex gap-6 text-sm","children":[["$","a",null,{"href":"/privacy","className":"text-muted-foreground hover:text-sky-500 transition-colors","children":"Privacy Policy"}],["$","a",null,{"href":"/terms","className":"text-muted-foreground hover:text-sky-500 transition-colors","children":"Terms of Service"}]]}]]}]]}]}]]}]}]}]]}],{"children":["__PAGE__",["$","$1","c",{"children":[["$","main",null,{"className":"relative overflow-hidden","children":[["$","$L7",null,{}],["$","$L8",null,{}],["$","$L9",null,{}],["$","section",null,{"id":"about","className":"snap-section min-h-screen relative","children":["$","$La",null,{}]}],["$","section",null,{"id":"skills","className":"snap-section min-h-screen relative","children":["$","$Lb",null,{}]}],["$","section",null,{"id":"projects","className":"snap-section min-h-screen relative","children":["$","$Lc",null,{"projects":[{"slug":"Portfolio","frontmatter":{"title":"Building My Portfolio in Two Days","description":"I built my personal portfolio website in two days, focusing on a clear and maintainable design without unnecessary complexity.","date":"22 Apr, 2025","coverImage":"/projects/Portfolio/portfolio_main.png","tags":["Web App","Next.js","AWS","Amplify","v0"],"liveUrl":"https://singbirds.net","githubUrl":"https://github.com/fairy-pitta/singbirds-portfolio","gallery":["/projects/Portfolio/portfolio_project.png","/projects/Portfolio/portfolio_skills.png","/projects/Portfolio/portfolio_mobile.png"]},"content":"$d"},{"slug":"SGBirdCall","frontmatter":{"title":"Learning Singapore‚Äôs Birds by Ear: A Look at the SingBirds Call Quiz","description":"An interactive quiz web app to test out your bird call knowledge","date":"Oct 15, 2024","coverImage":"/projects/callquiz/CallQuiz_main.png","tags":["React","Django","Birds"],"liveUrl":"https://quiz.singbirds.net/","githubUrl":"https://github.com/fairy-pitta/Singbirds-frontend","gallery":["/projects/callquiz/CallQuiz_correct.png","/projects/callquiz/CallQuiz_home.png","/projects/callquiz/CallQuiz_result.png","/projects/callquiz/CallQuiz_top.png"]},"content":"$e"}]}]}],["$","section",null,{"id":"blog","className":"snap-section min-h-screen relative","children":["$","$Lf",null,{"posts":[{"slug":"bird-sound-classification-in-singapore","frontmatter":{"title":"Singapore - Challenges and Progress in Bioacoustics","date":"May 6, 2025","excerpt":"A quick look at the bioacoustics challenges in Singapore","coverImage":"/blogs/bird_call_singapore.png","readTime":"10 min read","tags":["Deep Research","bioacoustics","Singapore"]},"content":"$10"},{"slug":"bird-sound-classification-in-data-sparse-regional-context","frontmatter":{"title":"Approaches Bioacoutics via Data","date":"May 5, 2025","excerpt":"Bird Sound Classification in Data-Sparse Regional Contexts: Literature Review","coverImage":"/blogs/bird_call_illustration.png","readTime":"20 min read","tags":["Deep Research","bioacoustics","data analysis"]},"content":"$11"},{"slug":"tools-for-bird-acoustic-monitoring","frontmatter":{"title":"New Technologies for Bird Acoustic Monitoring","date":"April 27, 2025","excerpt":"Bird monitoring tools are transforming conservation across Southeast Asia","coverImage":"/blogs/ecological-model-on-screen.png","readTime":"10 min read","tags":["Deep Research","bioacoustics"]},"content":"$12"},{"slug":"change-in-bird-vocalization","frontmatter":{"title":"Changes in Bird Vocalization in City vs. Forest","date":"April 26, 2025","excerpt":"A quick look at the changes in bird vocalizations between urban and forest","coverImage":"/blogs/bird_calling.JPG","readTime":"10 min read","tags":["Deep Research","bioacoustics"]},"content":"$13"}]}]}],["$","section",null,{"id":"contact","className":"snap-section min-h-screen relative","children":["$","$L14",null,{}]}]]}],"$undefined",null,["$","$L15",null,{"children":["$L16","$L17",null]}]]}],{},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","20wJPx5A997egG6T9E1wZ",{"children":[["$","$L18",null,{"children":"$L19"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}],["$","$L1a",null,{"children":"$L1b"}]]}],false]],"m":"$undefined","G":["$1c","$undefined"],"s":false,"S":true}
19:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
16:null
17:null
1b:[["$","title","0",{"children":"SingBirds | Environmental Tech Developer"}],["$","meta","1",{"name":"description","content":"Portfolio of SingBirds - Environmental Technologist and Developer"}],["$","meta","2",{"name":"application-name","content":"SingBirds"}],["$","meta","3",{"name":"author","content":"SingBirds"}],["$","meta","4",{"name":"generator","content":"v0.dev"}],["$","link","5",{"rel":"shortcut icon","href":"/favicon.ico"}],["$","link","6",{"rel":"icon","href":"/fairy-pitta.png"}],["$","link","7",{"rel":"apple-touch-icon","href":"/apple-icon.png"}]]
